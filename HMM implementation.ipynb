{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "annoying-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-seattle",
   "metadata": {},
   "source": [
    "## Setup Emission, Transition, and Pi Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "incomplete-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds for area we want to focus\n",
    "lower_bound = 0\n",
    "upper_bound = 500000\n",
    "\n",
    "# error for emission probability\n",
    "error = 0.01\n",
    "\n",
    "# using ceu to lower runtime since everything will take a while/lots of space\n",
    "ref_hap_file = \"1000GP_Phase3_yri_chr16.hap.gz\"\n",
    "impute_hap_file = \"ps2_impute.subset.gen.gz\"\n",
    "legend_file = \"1000GP_Phase3_chr16.legend.gz\"\n",
    "\n",
    "# read in locations and haplotypes\n",
    "ref_haplotypes = gzip.open(ref_hap_file, 'rb')\n",
    "legend = gzip.open(legend_file, 'rb')\n",
    "imp_haplotypes = gzip.open(impute_hap_file, 'rb')\n",
    "\n",
    "# get all known positions and haplotypes in dataset we are imputing in\n",
    "# format: chr id pos ref_allele alt_allele hom_ref het hom_alt (the last 3 will have a 1 for which is true)\n",
    "known_pos = {}\n",
    "obs_gts = []\n",
    "for known_hap in imp_haplotypes:\n",
    "    gt = known_hap.decode('utf-8').strip().split(' ')\n",
    "    if int(gt[2]) < lower_bound: continue\n",
    "    if int(gt[2]) > upper_bound: break\n",
    "    known_pos[gt[2]] = 1\n",
    "    if gt[5] == 1:\n",
    "        obs_gts.append(0)\n",
    "    elif gt[6] == 1:\n",
    "        obs_gts.append(1)\n",
    "    else:\n",
    "        obs_gts.append(2)\n",
    "\n",
    "legend_metadata = []\n",
    "filtered_haplotypes = []\n",
    "\n",
    "# skip header\n",
    "legend.readline()\n",
    "\n",
    "# remember all of the biallelic sites that are unknown\n",
    "unknown_sites = []\n",
    "unknown_haps = []\n",
    "\n",
    "# filter out all indices if they aren't known\n",
    "# take the pos, allele0, and allele1 of metadata\n",
    "for hap_line, meta_line in zip(ref_haplotypes, legend):\n",
    "    metadata = meta_line.decode('utf-8').strip().split(' ')\n",
    "    if int(metadata[1]) < lower_bound: continue\n",
    "    if int(metadata[1]) > upper_bound: break\n",
    "    if not known_pos.get(metadata[1], False):\n",
    "        if \"Biallelic\" in metadata[4]:\n",
    "            unknown_sites.append(metadata[1])\n",
    "            unknown_haps.append(metadata[1:4]+[int(i) for i in hap_line.decode('utf-8').strip().split(' ')])\n",
    "        continue\n",
    "    haplotype = [int(i) for i in hap_line.decode('utf-8').strip().split(' ')]\n",
    "    legend_metadata.append(metadata[1:4])\n",
    "    filtered_haplotypes.append(haplotype)\n",
    "\n",
    "ref_haplotypes.close()\n",
    "legend.close()\n",
    "imp_haplotypes.close()\n",
    "\n",
    "assert len(filtered_haplotypes) == len(obs_gts)\n",
    "\n",
    "# create pairs of haplotypes\n",
    "H = len(filtered_haplotypes[0])\n",
    "hap_pairs = int((H*(H+1))/2)\n",
    "\n",
    "# create dictionary that can convert from number to pair of indices\n",
    "ind = 0\n",
    "start = 0\n",
    "convert_to_pair = {}\n",
    "for pos1 in range(start, H):\n",
    "    for pos2 in range(pos1, H):\n",
    "        convert_to_pair[ind] = (pos1, pos2)\n",
    "        ind += 1\n",
    "    start += 1\n",
    "\n",
    "assert ind == hap_pairs\n",
    "\n",
    "# emission[x,y] = P(observed gt at col y|haplotype pair x)\n",
    "emission = np.zeros((hap_pairs, len(filtered_haplotypes)))\n",
    "for hap_pair in range(hap_pairs):\n",
    "    for pos in range(len(filtered_haplotypes)):\n",
    "        i, j = convert_to_pair[hap_pair]\n",
    "        \n",
    "        # grab reference genotype of the haplotype pair chosen i, j\n",
    "        ref_gt = (filtered_haplotypes[pos][i] + filtered_haplotypes[pos][j])\n",
    "        \n",
    "        # generate emission probability\n",
    "        if obs_gts[pos] == ref_gt and ref_gt == 1:\n",
    "            emission[hap_pair, pos] = (1-error)**2 + error**2\n",
    "        elif not (obs_gts[pos] == ref_gt) and (ref_gt == 1):\n",
    "            emission[hap_pair, pos] = 2*(1-error)*error\n",
    "        elif obs_gts[pos] == ref_gt and ref_gt%2==0:\n",
    "            emission[hap_pair, pos] = (1-error)**2\n",
    "        elif obs_gts[pos] == 1 and ref_gt%2 == 0:\n",
    "            emission[hap_pair, pos] = (1-error)*error\n",
    "        else:\n",
    "            emission[hap_pair, pos] = error**2\n",
    "\n",
    "            \n",
    "# transition mtx where every changing pairs is slightly penalized and staying is same\n",
    "# starting pi values are all equal (couldn't find actual values based on LD)\n",
    "transition = np.ones((hap_pairs, hap_pairs))\n",
    "for i in range(0, transition.shape[0]):\n",
    "    transition[i,i] += 1\n",
    "transition = transition/(hap_pairs+1)\n",
    "pi = np.ones((hap_pairs, ))/hap_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-default",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cutting-algeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2526, 2526, 2526, 2526, 2526, 1491, 1070, 1070, 1070, 1070, 858, 858, 858, 858, 858, 858, 216, 216, 216, 0, 0, 0, 1908, 1908, 1908, 1908, 1908, 1908, 216, 216, 1908, 1908, 216, 431, 2526, 2526, 645, 645, 645, 645, 216, 7146, 7146, 0, 0, 0, 0, 858, 858, 858, 858, 15308, 15308, 15308, 15308, 15308, 2526, 0, 0, 645, 645, 7683, 7683, 7683, 7683, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# viterbi algorithm\n",
    "# to perform the algorithm act as if the only locations are the known ones and\n",
    "# then when all states are found go back and determine what haplotype based on nearest SNPS\n",
    "viterbi = np.zeros((emission.shape[0], emission.shape[1]))\n",
    "viterbi[:,0] = np.multiply(pi, emission[:,0])\n",
    "\n",
    "# store what state the new probability came from for path\n",
    "backtrack = np.zeros((emission.shape[0], emission.shape[1]))\n",
    "for t in range(1, emission.shape[1]):\n",
    "    for state in range(hap_pairs):\n",
    "        # all possible probabilities at a certain haplotype \n",
    "        possible_probs = emission[state,t]*transition[state,:]*viterbi[:,t-1]\n",
    "        # calculate max \n",
    "        viterbi[state,t] = max(possible_probs)\n",
    "        # store what state came from\n",
    "        backtrack[state,t] = np.argmax(possible_probs)\n",
    "\n",
    "# backtrack to find all optimal states\n",
    "path = []\n",
    "last_best_state = np.argmax(viterbi[:,-1])\n",
    "path.append(last_best_state)\n",
    "for t in range(emission.shape[1]-1, 0,-1):\n",
    "    path.insert(0, int(backtrack[last_best_state,t]))\n",
    "    last_best_state = int(backtrack[last_best_state,t])\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-sending",
   "metadata": {},
   "source": [
    "## Recover Haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "auburn-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess all of the unknowns from unknown_sites = []\n",
    "# from path get locations \n",
    "\n",
    "# find closest known site for the current unknown site\n",
    "known_keys = sorted([int(key) for key in list(known_pos.keys())])\n",
    "closest = 0\n",
    "imputed_gts = []\n",
    "\n",
    "for i, pos in enumerate(unknown_sites):\n",
    "    if closest == len(known_keys)-1:\n",
    "        # get coords of haplotype pair\n",
    "        coord1, coord2 = convert_to_pair[path[closest]]\n",
    "        # update offset from inserting position and ref/alt alleles\n",
    "        coord1 += 3\n",
    "        coord2 += 3\n",
    "        # output imputed genotype\n",
    "        imputed_gt = unknown_haps[i][coord1] + unknown_haps[i][coord2]\n",
    "        imputed_gts.append(unknown_haps[i][:3]+[imputed_gt])\n",
    "        continue\n",
    "    \n",
    "    current_dist = np.abs(int(pos) - known_keys[closest])\n",
    "    next_dist = np.abs(int(pos) - known_keys[closest+1])\n",
    "    # find closest known haplotype pair \n",
    "    while current_dist > next_dist:\n",
    "        closest += 1\n",
    "        if closest == len(known_keys) - 1: break\n",
    "        current_dist = np.abs(int(pos) - known_keys[closest])\n",
    "        next_dist = np.abs(int(pos) - known_keys[closest+1])\n",
    "    # output genotype\n",
    "    coord1, coord2 = convert_to_pair[path[closest]]\n",
    "    # update offset from inserting position and ref/alt alleles\n",
    "    coord1 += 3\n",
    "    coord2 += 3\n",
    "    # output imputed genotype\n",
    "    imputed_gt = unknown_haps[i][coord1] + unknown_haps[i][coord2]\n",
    "    imputed_gts.append(unknown_haps[i][:3]+[imputed_gt])\n",
    "    \n",
    "known_gts = []\n",
    "for ind in range(len(legend_metadata)):\n",
    "    known_gts.append(legend_metadata[ind]+[obs_gts[ind]])\n",
    "all_gts = imputed_gts + known_gts\n",
    "all_gts.sort(key=lambda x: int(x[0]))\n",
    "\n",
    "# write imputed genotypes to file\n",
    "output = open(\"./my_impute_alg_yri.txt\", 'a')\n",
    "output.write(\"chr\\tpos\\tref\\talt\\tgenotype\\n\")\n",
    "for genotype in all_gts:\n",
    "    output.write(\"%s\\t%s\\t%s\\t%s\\t%i\\n\"%(\"16\", genotype[0], genotype[1], genotype[2], genotype[3]))\n",
    "    \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-rotation",
   "metadata": {},
   "source": [
    "## Run impute 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "popular-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      " IMPUTE version 2.3.2 \n",
      "======================\n",
      "\n",
      "Copyright 2008 Bryan Howie, Peter Donnelly, and Jonathan Marchini\n",
      "Please see the LICENCE file included with this program for conditions of use.\n",
      "\n",
      "The seed for the random number generator is 1754299535.\n",
      "\n",
      "Command-line input: impute2 -m /datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/genetic_map_chr16_combined_b37.txt -h /datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/1000GP_Phase3_yri_chr16.hap.gz -l /datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/1000GP_Phase3_chr16.legend.gz -phase -Ne 20000 -g /datasets/cs284-sp21-A00-public/ps2/ps2_impute.subset.gen.gz -int 0 5e5 -o ./ps2_impute.phased.yri.impute2\n",
      "\n",
      "---------------------------------\n",
      " Nomenclature and data structure \n",
      "---------------------------------\n",
      "\n",
      "     Panel 0: phased reference haplotypes\n",
      "     Panel 2: unphased study genotypes\n",
      "\n",
      "For optimal results, each successive panel (0,1,2) should contain a subset of the SNPs in the previous panel. When the data structure deviates from this ideal configuration, IMPUTE2 tries to use as much of the available information as possible; see documentation for details.\n",
      "\n",
      "-------------\n",
      " Input files \n",
      "-------------\n",
      "\n",
      "         Panel 0 haplotypes: /datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/1000GP_Phase3_yri_chr16.hap.gz\n",
      "         Panel 0 hap legend: /datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/1000GP_Phase3_chr16.legend.gz\n",
      "          Panel 2 genotypes: /datasets/cs284-sp21-A00-public/ps2/ps2_impute.subset.gen.gz\n",
      "                genetic map: /datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/genetic_map_chr16_combined_b37.txt\n",
      "\n",
      "--------------\n",
      " Output files \n",
      "--------------\n",
      "\n",
      "                main output: ./ps2_impute.phased.yri.impute2\n",
      "                SNP QC info: ./ps2_impute.phased.yri.impute2_info\n",
      "             sample QC info: ./ps2_impute.phased.yri.impute2_info_by_sample\n",
      "                run summary: ./ps2_impute.phased.yri.impute2_summary\n",
      "                warning log: ./ps2_impute.phased.yri.impute2_warnings\n",
      "        Panel 2 phased haps: ./ps2_impute.phased.yri.impute2_haps\n",
      "   Panel 2 phase confidence: ./ps2_impute.phased.yri.impute2_haps_confidence\n",
      "\n",
      "-----------------\n",
      " Data processing \n",
      "-----------------\n",
      "\n",
      "-reading genetic map from -m file\n",
      " --filename=[/datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/genetic_map_chr16_combined_b37.txt]\n",
      " --read 386 SNPs in the analysis interval+buffer region\n",
      "\n",
      "-reading Panel 2 genotypes from -g file\n",
      " --filename=[/datasets/cs284-sp21-A00-public/ps2/ps2_impute.subset.gen.gz]\n",
      " --detected 1 individuals\n",
      " --read 230 SNPs in the analysis interval+buffer region\n",
      "\n",
      "-reading Panel 0 haplotypes from -h and -l files\n",
      " --filename=[/datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/1000GP_Phase3_yri_chr16.hap.gz]\n",
      " --filename=[/datasets/cs284-sp21-A00-public/ps2/imputation/1000GP_Phase3/1000GP_Phase3_chr16.legend.gz]\n",
      " --detected 216 haplotypes\n",
      " --read 27799 SNPs in the analysis interval+buffer region\n",
      "\n",
      "-removing SNPs that violate the hierarchical data requirements\n",
      " --no SNPs removed\n",
      "\n",
      "-removing reference-only SNPs from buffer region\n",
      " --removed 10171 SNPs\n",
      "\n",
      "-checking strand alignment between Panel 2 and Panel 0 by allele labels\n",
      " --flipped strand due to allele mismatch at 0 out of 230 SNPs in Panel 2\n",
      "\n",
      "-aligning allele labels between panels\n",
      "\n",
      "-removing non-aligned genotyped SNPs\n",
      " --removed 0 out of 230 SNPs with data in multiple panels\n",
      "\n",
      "--------------\n",
      " Data summary \n",
      "--------------\n",
      "\n",
      "[type 0 = SNP in Panel 0 only]\n",
      "[type 1 = SNP in Panel 1]\n",
      "[type 2 = SNP in Panel 2 and all ref panels]\n",
      "[type 3 = SNP in Panel 2 only]\n",
      "\n",
      "-Upstream buffer region\n",
      " --0 type 0 SNPs\n",
      " --0 type 1 SNPs\n",
      " --0 type 2 SNPs\n",
      " --0 type 3 SNPs\n",
      " --0 total SNPs\n",
      "\n",
      "-Downstream buffer region\n",
      " --0 type 0 SNPs\n",
      " --0 type 1 SNPs\n",
      " --67 type 2 SNPs\n",
      " --0 type 3 SNPs\n",
      " --67 total SNPs\n",
      "\n",
      "-Analysis region (as defined by -int argument)\n",
      " --17398 type 0 SNPs\n",
      " --0 type 1 SNPs\n",
      " --163 type 2 SNPs\n",
      " --0 type 3 SNPs\n",
      " --17561 total SNPs\n",
      "\n",
      "-Output file\n",
      " --17398 type 0 SNPs\n",
      " --0 type 1 SNPs\n",
      " --163 type 2 SNPs\n",
      " --0 type 3 SNPs\n",
      "\n",
      "-In total, 17628 SNPs will be used in the analysis, including 230 Panel 2 SNPs\n",
      "\n",
      "-making initial haplotype guesses for Panel 2 by phasing hets at random and imputing missing genotypes from allele freqs\n",
      "\n",
      "-setting storage space\n",
      "-setting mutation matrices\n",
      "-setting switch rates\n",
      "\n",
      "----------------\n",
      " Run parameters \n",
      "----------------\n",
      "\n",
      "        reference haplotypes: 216 [Panel 0]\n",
      "           study individuals: 1 [Panel 2]\n",
      "           sequence interval: [0,500000]\n",
      "                      buffer: 250 kb\n",
      "                          Ne: 20000\n",
      "           input call thresh: 0.900\n",
      "     burn-in MCMC iterations: 10\n",
      "       total MCMC iterations: 30 (20 used for inference)\n",
      "      HMM states for phasing: 80 [Panel 2]\n",
      "   HMM states for imputation: 216 [Panel 0->2]\n",
      "                active flags: <-phase>\n",
      "\n",
      "---------\n",
      " Run log \n",
      "---------\n",
      "\n",
      "MCMC iteration [1/30]\n",
      "\n",
      "MCMC iteration [2/30]\n",
      "\n",
      "MCMC iteration [3/30]\n",
      "\n",
      "RESETTING PARAMETERS FOR \"SURROGATE FAMILY\" MODELING\n",
      "-setting mutation matrices\n",
      "-setting switch rates\n",
      "\n",
      "MCMC iteration [4/30]\n",
      "\n",
      "MCMC iteration [5/30]\n",
      "\n",
      "MCMC iteration [6/30]\n",
      "\n",
      "MCMC iteration [7/30]\n",
      "\n",
      "MCMC iteration [8/30]\n",
      "\n",
      "MCMC iteration [9/30]\n",
      "\n",
      "MCMC iteration [10/30]\n",
      "\n",
      "MCMC iteration [11/30]\n",
      "\n",
      "MCMC iteration [12/30]\n",
      "\n",
      "MCMC iteration [13/30]\n",
      "\n",
      "MCMC iteration [14/30]\n",
      "\n",
      "MCMC iteration [15/30]\n",
      "\n",
      "MCMC iteration [16/30]\n",
      "\n",
      "MCMC iteration [17/30]\n",
      "\n",
      "MCMC iteration [18/30]\n",
      "\n",
      "MCMC iteration [19/30]\n",
      "\n",
      "MCMC iteration [20/30]\n",
      "\n",
      "MCMC iteration [21/30]\n",
      "\n",
      "MCMC iteration [22/30]\n",
      "\n",
      "MCMC iteration [23/30]\n",
      "\n",
      "MCMC iteration [24/30]\n",
      "\n",
      "MCMC iteration [25/30]\n",
      "\n",
      "MCMC iteration [26/30]\n",
      "\n",
      "MCMC iteration [27/30]\n",
      "\n",
      "MCMC iteration [28/30]\n",
      "\n",
      "MCMC iteration [29/30]\n",
      "\n",
      "MCMC iteration [30/30]\n",
      "\n",
      "\n",
      "diploid sampling success rate: 0.979\n",
      "\n",
      "haploid sampling success rate: (no haploid sampling performed)\n",
      "\n",
      "\n",
      "--------------------------------\n",
      " Imputation accuracy assessment \n",
      "--------------------------------\n",
      "\n",
      "The table below is based on an internal cross-validation that is performed during each IMPUTE2 run. For this analysis, the program masks the genotypes of one variant at a time in the study data (Panel 2) and imputes the masked genotypes by using the remaining study and reference data. The imputed genotypes are then compared with the original genotypes to produce the concordance statistics shown in the table. You can learn more about this procedure and the contents of the table at http://mathgen.stats.ox.ac.uk/impute/concordance_table_description.html.\n",
      "\n",
      "In the current analysis, IMPUTE2 masked, imputed, and evaluated 163 genotypes that were called with high confidence (maximum probability >= 0.90) in the Panel 2 input file (-g or -known_haps_g).\n",
      "\n",
      "When the masked study genotypes were imputed with reference data from Panel 0, the concordance between original and imputed genotypes was as follows:\n",
      "\n",
      "  Interval  #Genotypes %Concordance         Interval  %Called %Concordance\n",
      "  [0.0-0.1]          0          0.0         [ >= 0.0]   100.0         96.3\n",
      "  [0.1-0.2]          0          0.0         [ >= 0.1]   100.0         96.3\n",
      "  [0.2-0.3]          0          0.0         [ >= 0.2]   100.0         96.3\n",
      "  [0.3-0.4]          0          0.0         [ >= 0.3]   100.0         96.3\n",
      "  [0.4-0.5]          0          0.0         [ >= 0.4]   100.0         96.3\n",
      "  [0.5-0.6]          1          0.0         [ >= 0.5]   100.0         96.3\n",
      "  [0.6-0.7]          3         66.7         [ >= 0.6]    99.4         96.9\n",
      "  [0.7-0.8]          2         50.0         [ >= 0.7]    97.5         97.5\n",
      "  [0.8-0.9]          8         87.5         [ >= 0.8]    96.3         98.1\n",
      "  [0.9-1.0]        149         98.7         [ >= 0.9]    91.4         98.7\n",
      "\n",
      "-generating consensus haplotype estimates (minimizing switch error)\n",
      "\n",
      "Have a nice day!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Run impute 2 on the four ref panels (ceu, yri, ceu_yri, not_asw)\n",
    "# Hint: you'll need to tell it where to find: \n",
    "# - genotypes (from the \"subest\" file, already in gen file format required for IMPUTE2)\n",
    "# - tell it to only look in the interval 5e6 to 10e6\n",
    "# See https://mathgen.stats.ox.ac.uk/impute/impute_v2.html for more info on how to run\n",
    "\n",
    "# Remove the \"NotImplemented\" line below.\n",
    "# This gets inserted automatically for Python code\n",
    "# Since nbgrader doesn't know this is a bash cell\n",
    "\n",
    "# Example impute2 command: \n",
    "# impute2 -m genetic_map_chr${chrom}_combined_b37.txt \\\n",
    "#     -h 1000GP_Phase3_${refpanel}_chr${chrom}.hap.gz \\\n",
    "#     -l 1000GP_Phase3_chr${chrom}.legend.gz \\\n",
    "#     -phase -Ne 20000\n",
    "#     -g mygenotypes.gen\n",
    "#     -int <from> <to>\n",
    "#     -o ~/ps2/ps2_impute.phased.${refpanel}.impute2\n",
    "\n",
    "DATADIR=/datasets/cs284-sp21-A00-public/ps2\n",
    "chrom=16\n",
    "\n",
    "impute2 -m ${DATADIR}/imputation/1000GP_Phase3/genetic_map_chr${chrom}_combined_b37.txt \\\n",
    "    -h ${DATADIR}/imputation/1000GP_Phase3/1000GP_Phase3_yri_chr${chrom}.hap.gz \\\n",
    "    -l ${DATADIR}/imputation/1000GP_Phase3/1000GP_Phase3_chr${chrom}.legend.gz \\\n",
    "    -phase -Ne 20000 \\\n",
    "    -g ${DATADIR}/ps2_impute.subset.gen.gz \\\n",
    "    -int 0 5e5 \\\n",
    "    -o ./ps2_impute.phased.yri.impute2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-lebanon",
   "metadata": {},
   "source": [
    "## R Squared comparing my algorithm and impute2/beagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fuzzy-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YRI 0.3622466193358977 17234\n",
      "\n",
      "0 nan\n",
      "\n",
      "0.0001 nan\n",
      "\n",
      "0.001 3.0293678022431386e-07\n",
      "\n",
      "0.01 0.0008678433083900325\n",
      "\n",
      "0.05 0.01804026414804819\n",
      "\n",
      "0.1 0.017397477335801933\n",
      "\n",
      "0.2 0.05840825620118725\n",
      "\n",
      "0.3 0.0784191903710226\n",
      "\n",
      "0.4 0.10843253673849403\n",
      "\n",
      "0.5 0.13296404627425928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/opt/conda/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 pset2_impute_my_data.py \\\n",
    "  ./my_impute_alg_yri.txt \\\n",
    "  ./ps2_impute.heldout.gen.gz \\\n",
    "  ./1000GP_Phase3_chr16.legend.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "moving-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yri 0.884029481495664 17240\n",
      "\n",
      "0 nan\n",
      "\n",
      "0.0001 nan\n",
      "\n",
      "0.001 0.2459042146268713\n",
      "\n",
      "0.01 0.31978765385990243\n",
      "\n",
      "0.05 0.6770876833917581\n",
      "\n",
      "0.1 0.6597446162713401\n",
      "\n",
      "0.2 0.6866287155647207\n",
      "\n",
      "0.3 0.7227348133018558\n",
      "\n",
      "0.4 0.7659839068948173\n",
      "\n",
      "0.5 0.7938252329228875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/opt/conda/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 pset2_impute2_data.py \\\n",
    "  ./ps2_impute.phased.yri.impute2 \\\n",
    "  ./ps2_impute.heldout.gen.gz \\\n",
    "  ./1000GP_Phase3_chr16.legend.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-protest",
   "metadata": {},
   "source": [
    "## Table comparing Impute2 and my Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-garage",
   "metadata": {},
   "source": [
    "| Category |My Algorithm | Impute2 |\n",
    "|----|-----|------|\n",
    "| Memory | 4.3 Gb | 150 mB|\n",
    "| Runtime| 2.66 Hours | 5 seconds |\n",
    "| Accuracy | 0.3622466193358977 | 0.884029481495664 |\n",
    "| MAF 0.001 Accuracy | 3.0293678022431386e-07 | 0.2459042146268713 |\n",
    "| MAF 0.01 Accuracy | 0.0008678433083900325 | 0.31978765385990243 |\n",
    "| MAF 0.05 Accuracy | 0.01804026414804819 | 0.6770876833917581 |\n",
    "| MAF 0.1 Accuracy | 0.017397477335801933 | 0.6597446162713401 |\n",
    "| MAF 0.2 Accuracy | 0.05840825620118725 | 0.6866287155647207 |\n",
    "| MAF 0.3 Accuracy | 0.0784191903710226 | 0.7227348133018558 |\n",
    "| MAF 0.4 Accuracy | 0.10843253673849403 | 0.7659839068948173 |\n",
    "| MAF 0.5 Accuracy | 0.13296404627425928 | 0.7938252329228875 |\n",
    "\n",
    "This is using 0-500kbp as the region. Note the time is with Jupyter lab running extremely slow. 17234 total SNPs genotyped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-district",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
